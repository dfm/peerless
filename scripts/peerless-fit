#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import h5py
import emcee
import pickle
import argparse
import numpy as np
from emcee.utils import MPIPool


def lnprob(theta):
    if not compute_blob:
        return model.lnprob(theta, compute_blob=False)
    return model.lnprob(theta)


parser = argparse.ArgumentParser(description="model some light curves")

parser.add_argument("filename", help="the initialized pickle model")
parser.add_argument("--nwalkers", type=int, default=40,
                    help="the number of walkers")
parser.add_argument("--nburn", type=int, default=500,
                    help="the number of burn-in MCMC steps")
parser.add_argument("--burniter", type=int, default=2,
                    help="the number of burn-in iterations")
parser.add_argument("--nsteps", type=int, default=50000,
                    help="the number of production MCMC steps")

args = parser.parse_args()

with open(args.filename, "rb") as f:
    model = pickle.load(f)
compute_blob = len(model.fit_lcs) <= 2

# Initialize the MPI-based pool used for parallelization.
pool = MPIPool()

if not pool.is_master():
    # Wait for instructions from the master process.
    pool.wait()
    sys.exit(0)

basedir = os.path.split(args.filename)[0]

system = model.system
cols = system.get_parameter_names()
p0 = system.get_vector()
ndim, nwalkers = len(p0), args.nwalkers
sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, pool=pool)

# Set up the output.
chain_fn = os.path.join(basedir, "chain.h5")
with h5py.File(chain_fn, "w") as f:
    f.create_dataset("chain", shape=(args.nsteps, nwalkers),
                     dtype=[(k, np.float64) for k in cols])
    f.create_dataset("lnprob", shape=(args.nsteps, nwalkers))
    f.create_dataset("params", shape=(args.nsteps, nwalkers),
                     dtype=[("ncadence", np.int64),
                            ("period", np.float64),
                            ("impact", np.float64),
                            ("eccen", np.float64)])

    if compute_blob:
        n = sum(len(lc.time) for lc in model.fit_lcs)
        f.create_dataset("pred", shape=(args.nsteps, nwalkers, 2, n))

    data = np.array([
        (i, l.time[j], l.flux[j], l.ferr[j])
        for i, l in enumerate(model.fit_lcs) for j in range(len(l.time))
    ], dtype=[("chunk", np.int64), ("time", np.float64),
              ("flux", np.float64), ("ferr", np.float64)])
    f.create_dataset("data", data=data)

# Run the MCMC.
burniter = max(1, args.burniter)
for i in range(burniter):
    print("Burn-in: {0}".format(i+1))
    p0 = p0[None, :] + 1e-6 * np.random.randn(nwalkers, ndim)

    ret = sampler.run_mcmc(p0, args.nburn)
    p0 = ret[0]
    if i < burniter - 1:
        p0 = sampler.flatchain[np.argmax(sampler.flatlnprobability)]

print("Production")
sampler.reset()
for i, ret in enumerate(sampler.sample(p0, iterations=args.nsteps,
                                       storechain=False)):
    pos, lnp, _, blob = ret
    with h5py.File(chain_fn, "a") as f:
        for n in range(nwalkers):
            for j, k in enumerate(cols):
                f["chain"][i, n, k] = pos[n, j]

            f["params"][i, n, "ncadence"] = blob[n][1]
            f["params"][i, n, "period"] = blob[n][0][0]
            f["params"][i, n, "eccen"] = blob[n][0][1]
            f["params"][i, n, "impact"] = blob[n][0][2]

            if compute_blob:
                f["pred"][i, n, 0] = np.concatenate([b[0] for b in blob[n][2]])
                f["pred"][i, n, 1] = np.concatenate([b[1] for b in blob[n][2]])

        f["lnprob"][i] = lnp
        f.attrs["step"] = i

# Close the processes.
pool.close()
