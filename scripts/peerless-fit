#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import h5py
import emcee
import pickle
import argparse
import numpy as np
from mpipool import MPIPool


def lnprob(theta):
    if not compute_blob:
        return model.lnprob(theta, compute_blob=False)
    return model.lnprob(theta)


parser = argparse.ArgumentParser(description="model some light curves")

parser.add_argument("filename", help="the initialized pickle model")
parser.add_argument("--nwalkers", type=int, default=40,
                    help="the number of walkers")
parser.add_argument("--clobber", action="store_true",
                    help="clobber existing output")
parser.add_argument("--nburn", type=int, default=500,
                    help="the number of burn-in MCMC steps")
parser.add_argument("--burniter", type=int, default=2,
                    help="the number of burn-in iterations")
parser.add_argument("--nsteps", type=int, default=50000,
                    help="the number of production MCMC steps")

args = parser.parse_args()

with open(args.filename, "rb") as f:
    model = pickle.load(f)
model.system.central.dilution = 0.0
model.system.freeze_parameter("central:dilution")
compute_blob = True  # len(model.fit_lcs) <= 2

# Initialize the MPI-based pool used for parallelization.
with MPIPool() as pool:
    if not pool.is_master():
        # Wait for instructions from the master process.
        pool.wait()
        sys.exit(0)

    basedir = os.path.split(args.filename)[0]

    system = model.system
    cols = system.get_parameter_names()

    # Set up the output.
    chain_fn = os.path.join(basedir, "chain.h5")
    if args.clobber or not os.path.exists(chain_fn):
        p0 = system.get_vector()
        m = np.isfinite(p0)
        while not np.all(m):
            p0[np.isnan(p0)] = 0.0
            p0[(p0 < 0.0) & (~m)] = -10.0
            p0[(p0 > 0.0) & (~m)] = 10.0
            m = np.isfinite(p0)

        ndim, nwalkers = len(p0), args.nwalkers
        while nwalkers <= ndim * 2:
            nwalkers += args.nwalkers
        sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, pool=pool)

        with h5py.File(chain_fn, "w") as f:
            for k, v in model.spec.items():
                f.attrs[k] = v
            f.create_dataset("chain", shape=(args.nsteps, nwalkers),
                             dtype=[(k, np.float64) for k in cols])
            f.create_dataset("lnprob", shape=(args.nsteps, nwalkers))
            f.create_dataset("lnprior", shape=(args.nsteps, nwalkers))
            f.create_dataset("params", shape=(args.nsteps, nwalkers),
                             dtype=[("ncadence", np.int64),
                                    ("period", np.float64),
                                    ("impact", np.float64),
                                    ("eccen", np.float64)])

            # if compute_blob:
            #     n = sum(len(lc.time) for lc in model.fit_lcs)
            #     f.create_dataset("pred", shape=(args.nsteps, nwalkers, 2, n))

            data = np.array([
                (i, l.time[j], l.flux[j], l.ferr[j])
                for i, l in enumerate(model.fit_lcs)
                for j in range(len(l.time))
            ], dtype=[("chunk", np.int64), ("time", np.float64),
                      ("flux", np.float64), ("ferr", np.float64)])
            f.create_dataset("data", data=data)

        # Run the MCMC.
        burniter = max(1, args.burniter)
        for i in range(burniter):
            print("Burn-in: {0}".format(i+1))
            coord = np.array(p0)
            p0 = p0[None, :] + 1e-6 * np.random.randn(nwalkers, ndim)

            # Don't allow -inf initialization.
            lp = np.array([lnprob(v)[0] for v in p0])
            m = ~np.isfinite(lp)
            while np.any(m):
                p0[m] = coord[None, :] + 1e-6 * np.random.randn(m.sum(), ndim)
                lp[m] = np.array([lnprob(v)[0] for v in p0[m]])
                m[m] = ~np.isfinite(lp)

            ret = sampler.run_mcmc(p0, args.nburn)
            p0 = ret[0]
            if i < burniter - 1:
                p0 = sampler.flatchain[np.argmax(sampler.flatlnprobability)]

        i0 = 0

    else:
        print("restarting...")
        with h5py.File(chain_fn, "a") as f:
            # del f["chain"]
            # f["chain"] = f["chain_2"]
            # del f["chain_2"]

            n = f.attrs["step"]
            p0 = np.array([f["chain"][n][k] for k in cols]).T
            nwalkers, ndim = p0.shape
            ntot = n + 1 + args.nsteps

            # Resize.
            ds = ["chain", "lnprob", "lnprior", "params"]
            # if compute_blob:
            #     ds += ["pred"]
            for k in ds:
                k2 = k + "_2"
                f[k2] = f[k]
                del f[k]
                s = list(f[k2].shape)
                s[0] = ntot
                f.create_dataset(k, shape=s, dtype=f[k2].dtype)
                f[k][:n+1] = f[k2][:n+1]
                del f[k2]

        sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, pool=pool)
        i0 = n + 1

    print("Production")
    sampler.reset()
    for i1, ret in enumerate(sampler.sample(p0, iterations=args.nsteps,
                                            storechain=False)):
        i = i0 + i1
        pos, lnp, _, blob = ret
        with h5py.File(chain_fn, "a") as f:
            for n in range(nwalkers):
                for j, k in enumerate(cols):
                    f["chain"][i, n, k] = pos[n, j]

                f["params"][i, n, "ncadence"] = blob[n][1]
                f["params"][i, n, "period"] = blob[n][0][0]
                f["params"][i, n, "eccen"] = blob[n][0][1]
                f["params"][i, n, "impact"] = blob[n][0][2]

                f["lnprior"][i, n] = blob[n][3]

                # if compute_blob:
                #     f["pred"][i, n, 0] = np.concatenate([
                #         b[0] for b in blob[n][2]
                #     ])
                #     f["pred"][i, n, 1] = np.concatenate([
                #         b[1] for b in blob[n][2]
                #     ])

            f["lnprob"][i] = lnp
            f.attrs["step"] = i
